# -*- coding: utf-8 -*-
"""iml6-10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OGchnmYi_y85_idz8Z26_f5qOxQpaEBF

expectation maximization
"""

pip install scikit-learn matplotlib

import numpy as np
from sklearn.mixture import GaussianMixture
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(42)
data = np.concatenate([np.random.normal(0, 1, 300), np.random.normal(5, 1, 200)])
data = data.reshape(-1, 1)

# Plot the histogram of the data
plt.hist(data, bins=50, density=True, alpha=0.5, color='b')

# Fit a Gaussian Mixture Model using Expectation-Maximization
gmm = GaussianMixture(n_components=2, random_state=42)

# Reshape the data to be a column vector
data = data.reshape(-1, 1)

# Fit the GMM to the data
gmm.fit(data)

# Plot the fitted Gaussian distributions
x = np.linspace(-5, 10, 1000).reshape(-1, 1)
y = np.exp(gmm.score_samples(x))
plt.plot(x, y, '--', label='Gaussian Mixture Model')

plt.xlabel('Value')
plt.ylabel('Density')
plt.legend()
plt.show()

"""maximum forest"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.datasets import load_iris

# Load iris dataset as an example
iris = load_iris()
X = iris.data
y = iris.target

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
confusion_mat = confusion_matrix(y_test, y_pred)
classification_rep = classification_report(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print("Confusion Matrix:\n", confusion_mat)
print("Classification Report:\n", classification_rep)

"""kmean"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# Generate a sample dataset
X, y = make_blobs(n_samples=300, centers=4, random_state=42)

# Visualize the dataset
plt.scatter(X[:, 0], X[:, 1], s=50, cmap='viridis')
plt.title('Generated Dataset')
plt.show()

# Apply K-means clustering
kmeans = KMeans(n_clusters=4, random_state=42)
kmeans.fit(X)

# Get cluster centers and labels
cluster_centers = kmeans.cluster_centers_
labels = kmeans.labels_

# Visualize the clustered dataset
plt.scatter(X[:, 0], X[:, 1], c=labels, s=50, cmap='viridis')
plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], c='red', marker='X', s=200, label='Centroids')
plt.title('K-means Clustering')
plt.legend()
plt.show()

"""apriori"""

from itertools import chain, combinations
from collections import defaultdict

def generate_candidates(itemsets, k):
    candidates = set()
    for itemset1 in itemsets:
        for itemset2 in itemsets:
            union_set = itemset1.union(itemset2)
            if len(union_set) == k:
                candidates.add(union_set)
    return candidates

def prune(itemsets, candidates):
    pruned_candidates = set()
    for candidate in candidates:
        subsets = list(combinations(candidate, len(candidate) - 1))
        if all(set(subset) in itemsets for subset in subsets):
            pruned_candidates.add(candidate)
    return pruned_candidates

def apriori(data, min_support):
    itemsets = [frozenset([item]) for item in set(chain(*data))]
    frequent_itemsets = []

    k = 2
    while itemsets:
        candidates = generate_candidates(itemsets, k)
        counts = defaultdict(int)
        for transaction in data:
            for candidate in candidates:
                if candidate.issubset(transaction):
                    counts[candidate] += 1

        frequent_itemsets.extend([itemset for itemset, count in counts.items() if count >= min_support])
        itemsets = prune(frequent_itemsets, generate_candidates(frequent_itemsets, k + 1))
        k += 1

    return frequent_itemsets

# Example usage:
data = [
    {'A', 'B', 'C'},
    {'A', 'C'},
    {'B', 'C'},
    {'A', 'B'},
    {'D'},
]

min_support = 2
result = apriori(data, min_support)
print("Frequent Itemsets:")
for itemset in result:
    print(f"{itemset}: {data.count(list(itemset))}")

"""pca"""

import numpy as np
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
import matplotlib.pyplot as plt

# Load iris dataset as an example
iris = load_iris()
X = iris.data
y = iris.target

# Standardize the data (optional but recommended for PCA)
mean = np.mean(X, axis=0)
std = np.std(X, axis=0)
X_standardized = (X - mean) / std

# Create a PCA instance and fit the data
pca = PCA(n_components=2)
principal_components = pca.fit_transform(X_standardized)

# Visualize the results
plt.figure(figsize=(8, 6))
plt.scatter(principal_components[:, 0], principal_components[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)
plt.title('PCA of Iris Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()